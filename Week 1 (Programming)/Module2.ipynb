{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy and pandas library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series\n",
    "\n",
    "One dimensional labeled array in Pandas series is capable of holding any type of data. Data alignment in Pandas is intrinsic. Why is pandas similar to NumPy on this regard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding what goes into a pandas series (notice the index input option for a series)\n",
    "pd.Series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas series from a list (similar to numpy.array)\n",
    "my_list = [1,2,3]\n",
    "print(my_list)\n",
    "ds = pd.Series(my_list) # short for data-series (ds)\n",
    "type(ds) # printing the type of pandas data series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the output (notice how the output is different than that of the numpy array)\n",
    "print(pd.Series(my_list))\n",
    "print(\"--\"*10)\n",
    "np.array(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, the syntax for creating a pandas series is pd.Series(data,index)\n",
    "\n",
    "# Creating a pandas series from numpy array\n",
    "my_array = np.linspace(0,10,11)\n",
    "ds = pd.Series(my_array)\n",
    "print(my_array)\n",
    "print(80*'-')\n",
    "ds # Pandas series prints the index and values (unlike numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with the index\n",
    "my_array = np.linspace(0,10,11)\n",
    "my_index1 = np.linspace(10,20,11)\n",
    "my_index2 = np.linspace(100,110,11)\n",
    "ds1 = pd.Series(data=my_array,index=my_index1) \n",
    "ds2 = pd.Series(data=my_array,index=my_index2) # Two series with same vlaues but different index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can not compare the two dataframes (due to different indexes they are two different dataframes)\n",
    "# For two pandas series to be comparable, they must be identical in all respects (index and values)\n",
    "ds1 == ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index can take any name (not only restricted to numbers)\n",
    "my_array3 = np.array([1,2,3,4])\n",
    "my_index3 = [\"A B C D\".split()]\n",
    "ds3 = pd.Series(data=my_array3,index=my_index3)\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we have the same index across two rows?\n",
    "my_array3 = np.array([1,2,3,4])\n",
    "my_index3 = [\"A B B C\".split()]\n",
    "ds3 = pd.Series(data=my_array3,index=my_index3)\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when we select index \"B\"\n",
    "ds3.loc[\"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two pandas series with different indices \n",
    "my_array3 = np.array([1,2,3,4])\n",
    "my_index3 = [\"A B C D\".split()]\n",
    "ds3 = pd.Series(data=my_array3,index=my_index3)\n",
    "\n",
    "my_array4 = np.array([1,2,3,4])\n",
    "my_index4 = [\"B C D E\".split()]\n",
    "ds4 = pd.Series(data=my_array4,index=my_index4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we try to add two series with different index values\n",
    "ds3+ds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is NaN (arithmetic operations on nan)\n",
    "np.nan\n",
    "#type(np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetting index (a new object from a new class)\n",
    "print(type(ds))\n",
    "print(ds)\n",
    "ds.reset_index()\n",
    "#type(ds.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Data Frames\n",
    "\n",
    "Recall that a numpy matrix was created by stacking numpy arrays. Similarly, a Pandas dataframe is constructed by stacking pandas series together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing our data through a numpy matrix first (numpy matrix is a stack of numpy ndarrays)\n",
    "my_matrix = np.array([['Ronaldo', 2002, 'Brazil'],\n",
    "                      ['Grosso', 2006, 'Italy'],\n",
    "                      ['Iniesta', 2010, 'Spain'],\n",
    "                      ['Gotze', 2014, 'Germany']])\n",
    "\n",
    "my_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas dataframe from scratch \n",
    "df = pd.DataFrame([['Andy', 87, 'M'],\n",
    "                   ['Bill', 67, 'M'],\n",
    "                   ['Catherine', 93, 'F'],\n",
    "                   ['David', 95, 'M'],\n",
    "                   ['Emma', 50, 'F']],\n",
    "                  columns=['Name', 'Score', 'Gender'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a column\n",
    "df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL way of selecting data (not the recommended way - not considered good a programming practice)\n",
    "df.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting two columns (spot the difference)\n",
    "df[['Name','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging the column order\n",
    "df = df[['Name','Gender','Score']] # not an inplace operation\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location operator - syntax is .loc[row,column]\n",
    "df.loc[0,[\"Name\",\"Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying data using the Location operator\n",
    "df.loc[0,\"Score\"] = 92\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the index and using the loc operator\n",
    "df.index = \"A B C D E\".split()\n",
    "df\n",
    "df.loc[\"A\",:]\n",
    "# error if we change A to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using the integer location operator - syntax .iloc[row_integer,column_integer]\n",
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing column with iloc\n",
    "df.iloc[:,1] # Rows and columns in iloc have to be integers (names will give an error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When a series is compared to a constant (similar to numpy)\n",
    "df[df['Score']>90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column\n",
    "df = pd.DataFrame([['stock1',50,1000],\n",
    "                   ['stock2',75,900],\n",
    "                   ['stock3',125,400]],columns=['stock_name','price','shares'])\n",
    "df\n",
    "df['market_cap'] = df['price']*df['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum function can be called on the series (and on the dataframe)\n",
    "df['market_cap'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe column by broadcasting\n",
    "df['pe_ratio'] = 0\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the market_cap column\n",
    "df.drop(columns=['market_cap'],inplace=True) # inplace=False is default\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows\n",
    "df.drop(0) # not an inplace opearator\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataframe on outstanding shares\n",
    "df.sort_values(by=['shares'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting index\n",
    "df = df.set_index('stock_name')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe \n",
    "df = pd.DataFrame(np.linspace(0,8,9).reshape(3,3),columns=['A B C'.split()])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum function \n",
    "df.sum(axis=0,skipna=True,min_count=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading panel data through pandas\n",
    "df = pd.read_csv(\"stock_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing a sample of the dataframe (due to large size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting info on our dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique permnos\n",
    "df['permno'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique company names\n",
    "df['company_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how the date is initially loaded \n",
    "df['date'][0]\n",
    "#type(df['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas has a convenient hack for dates\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe date format\n",
    "df['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting datetime from one format to another\n",
    "df['date'].dt.strftime(\"%d-%m-%Y\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing dates in pandas\n",
    "pd.Timestamp(year=2013,month=12,day=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding days to a date (or hours/minutes etc)\n",
    "some_date = pd.Timestamp(year=2013,month=12,day=7)\n",
    "print(some_date)\n",
    "some_date + pd.Timedelta('3 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series of dates (a cool trick!)\n",
    "my_date = pd.to_datetime(pd.Series(['13 July 2013','5 May 2010','15 March 2020']))\n",
    "print(my_date)\n",
    "my_date + pd.tseries.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas date range to generate dates\n",
    "pd.date_range(start='01 Jan 2021', end='31 Jan 2021')\n",
    "#pd.date_range(start='01 Jan 2021', periods=18, freq= 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataframe again\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataframe types (from info)\n",
    "df.info() # observe that prices and returns are still objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting price and returns to numbers\n",
    "df['price'] = pd.to_numeric(df['price'],errors='coerce')\n",
    "df['total_returns'] = pd.to_numeric(df['total_returns'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking info again\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows that have NaNs\n",
    "df.isnull() # Returns True in dataframe where values are NaN\n",
    "#df.isnull().sum() gives count of NaN values\n",
    "#df[df['total_returns'].isnull()] # notice I can pass a true/false array to see which rows have returns as missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaN values (involves dropping the whole row) - we drop the NaNs by writing command dropna()\n",
    "df.dropna().tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 for dealing with all NaN values\n",
    "df.replace(np.nan,0).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 for dealing with all NaN values\n",
    "df['total_returns'] = df['total_returns'].fillna(0)\n",
    "df['price'] = df['price'].fillna(method='ffill') # ffill is forward fill for handling NaN values\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "df = df[df['permno']!=10001] # df and index start without 10001 permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting your dataframe to a csv\n",
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with stock names and stock price\n",
    "df1 = pd.DataFrame([['A',100],\n",
    "                    ['B',120],\n",
    "                    ['C',50]],columns=['stock','price'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with stock name and stock PE ratio\n",
    "df2 = pd.DataFrame([['B',10],\n",
    "                    ['C',14],\n",
    "                    ['D',1000]],columns=['stock','pe_ratio'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple merging syntax\n",
    "pd.merge(left=df1,right=df2,on=['stock'],how='inner')\n",
    "# inner - intersection\n",
    "# outer - union\n",
    "# left - keeps all left dataframe \n",
    "# right - keeps all right dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "\n",
    "Similar to the Pivot table in excel, this function lets you group/split data for calculations and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the dataset analyzed above (shortcut to parse the dates while reading the data)\n",
    "df = pd.read_csv(\"stock_data.csv\",sep = ',',parse_dates=['date'])\n",
    "\n",
    "# Converting any letters into NaN\n",
    "df['price'] = pd.to_numeric(df['price'],errors='coerce')\n",
    "df['total_returns'] = pd.to_numeric(df['total_returns'],errors='coerce')\n",
    "\n",
    "# Creating a new column\n",
    "df['market cap'] = df['price']*df['share_outstanding']\n",
    "\n",
    "# Printing the tail\n",
    "df.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Company Name and counting number of data points available\n",
    "df.groupby('permno')['total_returns']#.mean().sort_values()\n",
    "#list(df.groupby('permno')['total_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating various statistics of returns for each PERMNO\n",
    "df.groupby('permno')['total_returns'].agg([np.mean,np.median,np.std,np.var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by dates and calculating average returns for each date\n",
    "df.groupby('date')['total_returns'].mean()#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning market cap >200,000 USD\n",
    "df.loc[df['market cap']<=200000,'category'] = 1\n",
    "df.loc[df['market cap']>200000,'category'] = 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by two columns\n",
    "df.groupby(['date','category'])['total_returns'].mean().head(15)#reset_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack/Unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example of unstacking (from above example)\n",
    "df.groupby(['date','category'])['total_returns'].mean().head(15)\n",
    "# Execute the unstack command and show how 1 and 2 are now separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new dataframe\n",
    "df2 = pd.DataFrame(np.linspace(0,10,10).reshape(5,2),columns=['A','B'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the columns \n",
    "df2.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Home Questions\n",
    "\n",
    "- **Pandas Mul Function** - Use this dataframe created in the code chunk below. We have three columns with random float values. The goal is to multiply columns 1 and 2 with columns 3. Firstly, notice how running the function (df[['col1','col2']]*df['col3']) returns a matrix of NaN values. Now, use the .mul() function by passing df[['col1','col2']].mul(df['col3'],axis=0). Notice how we get the same NaN matrix when we change axis=1 instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for problem 1\n",
    "np.random.seed(123)\n",
    "df = pd.DataFrame() # Creating a blank data-frame\n",
    "df['col1'] = np.random.rand(10)\n",
    "df['col2'] = np.random.rand(10)\n",
    "df['col3'] = np.random.rand(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load CSV data** (*stock_data.csv*) - parse dates using the *pd.to_datetime* command, read the stock returns as numeric values using the *pd.to_numeric* command, drop all NaN values using the dropna(), and reset the index. We will be working with this dataframe (unless stated otherwise) for the remainder of the questions below.\n",
    "\n",
    "- **Aggregating** - For each PERMNO, calculate the following statistics for the total_returns - sum, mean, std dev, and median. *Hint - use .groupby('permno') and then aggregate by the stats. The \".agg()\" function might be useful*.\n",
    "\n",
    "- **Filtering Text** - You can parse the company name column to search for companies that match your key words. A basic filter would be to check for \"OIL\", \"GAS\", and \"ENERGY\" companies. Write a program that returns the number of unique PERMNOs that match the above 3 key-words. *Hint*: Try using df['company_name'].str.contains(). Also, **regex** (regular expression) is a is a powerful library you may want to try out (for those interested in text analysis)!\n",
    "\n",
    "- **Merging** - Create a new dataframe with the last available date and price for each PERMNO. For example, the only entry for PERMNO 10001 is 31-Jul-2017 (date) and 12.95 (price). Merge this new dataframe onto the original dataframe. Fill the missing/NaN values using pandas fillna function with a method of your choice.\n",
    "\n",
    "- **Lagged Market Cap** - Create three new columns that contain the 6-month, 1-year, and 5-year lagged market cap. For example: PERMNO 10001 on 30-Jun-2016 will have the market cap as on 31-Dec-2015 (7.45 $\\cdot$ 10505 = 78262.25) as its 6-month lag market cap. Perform a similar exercise to generate the 1-year and the 5-year lag market caps.\n",
    "\n",
    "- **Resampling Frequency** - Convert the monthly dataframe that you loaded above into quarterly data and annual data. *Hint*: First, load and clean the data as per procedure mentioned above. Second, create a new column containing quarters corresponding to the given month (ex: 30-Nov-2010 is a quarter 4). Lastly, create a new dataframe \"df_quarter\" by aggregating data using the groupby command. For the quarterly dataframe your columns will be {permno, date, quarter, total_returns, market_cap}. Note that you will need to add the monthly returns to generate quarterly returns. Also note that you will need the quarter end market cap for each PERMNO each quarter. For example : In the Quarterly dataframe, PERMNO 10001 as of 31-Mar-2010 will have total returns 0.001055 (-0.018932-0.000656+0.020643) and the market cap 44351.37 (10.17$\\cdot$4361). Perform the same procedure for creating data at an annual frequency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
